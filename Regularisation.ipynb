{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4df2750-3f4a-419c-9a2a-5975f2e01122",
   "metadata": {},
   "source": [
    "## Mathematics behind regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095bdcd2-024f-4128-ad5f-d81844232404",
   "metadata": {},
   "source": [
    "**Step 1**: Initialization and SetupThe Concept:First, we define our class. The most important parameter here is lambda_param (often called $\\alpha$ in libraries like Scikit-Learn). This controls the \"strength\" of the regularization.\n",
    "If $\\lambda = 0$: We have standard Linear Regression.\n",
    "If $\\lambda$ is high: We heavily punish the model for complex weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07669eee-6b3e-4fd2-8a12-7e51b6254d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RegularizedLinearRegression:\n",
    "    def __init__(self, learning_rate=0.001, n_iterations=1000, lambda_param=0.1, mode='l2'):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.lambda_param = lambda_param\n",
    "        self.mode = mode  # 'l1' for Lasso, 'l2' for Ridge\n",
    "        self.weights = None\n",
    "        self.bias = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b60e89b-a52c-41d0-8a8e-d7c2d3532018",
   "metadata": {},
   "source": [
    "**Step 2**: The Prediction (Forward Pass)The Math:This part does not change. Regardless of regularization, the linear prediction formula remains:$$y = w \\cdot X + b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9f737b7-2cf8-43da-969c-46adf79dcd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6984f7cc-6b3a-4654-a685-7cd58339db61",
   "metadata": {},
   "source": [
    "**Step 3**: The Cost Function (The \"Why\")The Math:This is where we change the definition of \"Success.\" We don't just want low error; we want low error AND small weights.\n",
    "**Ridge (L2) Cost**:$$J(\\theta) = \\underbrace{\\frac{1}{2m} \\sum (y_{pred} - y)^2}_{\\text{MSE}} + \\underbrace{\\lambda \\sum \\theta^2}_{\\text{Penalty}}$$**Lasso (L1) Cost**:$$J(\\theta) = \\underbrace{\\frac{1}{2m} \\sum (y_{pred} - y)^2}_{\\text{MSE}} + \\underbrace{\\lambda \\sum |\\theta|}_{\\text{Penalty}}$$**Note**: We calculate this just to track progress. It is not strictly needed for the gradient descent update, but it's great for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4055f92-ceae-491f-9311-9af73fa970d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_cost(self, y_predicted, y, n_samples):\n",
    "        # 1. Standard Error (MSE)\n",
    "        cost = np.sum((y_predicted - y) ** 2) / (2 * n_samples)\n",
    "        \n",
    "        # 2. Add Regularization Penalty\n",
    "        if self.mode == 'l2':\n",
    "            penalty = self.lambda_param * np.sum(self.weights ** 2)\n",
    "        else: # l1\n",
    "            penalty = self.lambda_param * np.sum(np.abs(self.weights))\n",
    "            \n",
    "        return cost + penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90851a32-2c56-401d-8e99-04b36f1d4bae",
   "metadata": {},
   "source": [
    "**Step 4**: The Derivatives (The \"How\")\n",
    "We need to find the gradient (slope) to update our weights.When we take the derivative of the penalty terms, we get:Ridge Derivative: The derivative of $\\theta^2$ is $2\\theta$.$$dw = \\text{Standard Gradient} + 2\\lambda \\theta$$Lasso Derivative: The derivative of $|\\theta|$ is the sign of $\\theta$ (either +1 or -1).$$dw = \\text{Standard Gradient} + \\lambda \\cdot \\text{sign}(\\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51309282-5d91-452f-8f9f-292c8b34290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_gradients(self, X, y, y_predicted, n_samples):\n",
    "        # 1. Calculate Standard Gradient (dW)\n",
    "        dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "        db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "        \n",
    "        # 2. Add Regularization Term to dW\n",
    "        if self.mode == 'l2':\n",
    "            # Ridge: Derivative of w^2 is 2w\n",
    "            dw += (2 * self.lambda_param / n_samples) * self.weights\n",
    "            \n",
    "        elif self.mode == 'l1':\n",
    "            # Lasso: Derivative of |w| is sign(w)\n",
    "            dw += (self.lambda_param / n_samples) * np.sign(self.weights)\n",
    "            \n",
    "        # Note: We rarely regularize the bias (db), so we leave it alone.\n",
    "        return dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9523b9-8358-4f2d-997f-cd7086edcfed",
   "metadata": {},
   "source": [
    "**Step 5**: The Training Loop (Putting it Together)The Concept:Finally, we run the loop. We predict, check gradients, and update weights.$$\\theta_{new} = \\theta_{old} - \\text{learning\\_rate} \\times \\text{gradient}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ea56270-1418-43bd-a0bd-59a3d578ea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        for _ in range(self.n_iterations):\n",
    "            # 1. Predict\n",
    "            y_predicted = np.dot(X, self.weights) + self.bias\n",
    "            \n",
    "            # 2. Calculate Gradients (with regularization)\n",
    "            dw, db = self._get_gradients(X, y, y_predicted, n_samples)\n",
    "            \n",
    "            # 3. Update Weights\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5291d9bb-5994-4155-a001-0ac0cc966aa5",
   "metadata": {},
   "source": [
    "**Conclusion**: The \"Tax\" on Complexity\n",
    "In this notebook, we built Regularised Linear Regression from scratch. We discovered that regularisation is essentially adding a \"tax\" to our model's learning process.\n",
    "- Standard Regression cares only about getting the answer right (minimising error).\n",
    "- Regularised Regression cares about getting the answer right simply.\n",
    "\n",
    "By adding the penalty term ($\\lambda$) to our cost function, we force the model to choose:\n",
    "- is it worth increasing this weight to lower the error slightly, or is the \"tax\" (penalty) too high? Which one should you choose?\n",
    "- Use Ridge (L2) when you want to keep all your features but reduce the impact of noise. It shrinks weights close to 0 but rarely exactly to 0.\n",
    "- Use Lasso (L1) when you suspect many features are useless. It can shrink weights all the way to 0, effectively deleting bad features from your equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162817f1-4dcd-47a8-81de-4094442120e2",
   "metadata": {},
   "source": [
    "**The Implementation Example**\n",
    "The code below demonstrates how to actually run the class we wrote. I have commented on every single \"knob\" (variable) you can turn so you can explain exactly how tuning works to your audience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "870a2a37-f678-49e2-924c-8a047a0901ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RegularizedLinearRegression:\n",
    "    def __init__(self, learning_rate=0.001, n_iterations=1000, lambda_param=0.1, mode='l2'):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.lambda_param = lambda_param\n",
    "        self.mode = mode  # 'l1' for Lasso, 'l2' for Ridge\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "    def _get_gradients(self, X, y, y_predicted, n_samples):\n",
    "        # 1. Calculate Standard Gradient (dW)\n",
    "        dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
    "        db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "        \n",
    "        # 2. Add Regularisation Term to dW\n",
    "        if self.mode == 'l2':\n",
    "            # Ridge: Derivative of w^2 is 2w\n",
    "            dw += (2 * self.lambda_param / n_samples) * self.weights\n",
    "            \n",
    "        elif self.mode == 'l1':\n",
    "            # Lasso: Derivative of |w| is sign(w)\n",
    "            dw += (self.lambda_param / n_samples) * np.sign(self.weights)\n",
    "            \n",
    "        return dw, db\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        for _ in range(self.n_iterations):\n",
    "            # 1. Predict\n",
    "            y_predicted = np.dot(X, self.weights) + self.bias\n",
    "            \n",
    "            # 2. Calculate Gradients (with regularisation)\n",
    "            dw, db = self._get_gradients(X, y, y_predicted, n_samples)\n",
    "            \n",
    "            # 3. Update Weights\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3b95d47-4dd0-41ef-aa76-8860aa37d01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Final Weights: [1.86079214]\n",
      "Final Bias: 1.3889763125214563\n",
      "Prediction for input 6: [12.55372914]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Note: Ensure the 'RegularizedLinearRegression' class is defined in a previous cell\n",
    "\n",
    "# --- STEP 1: GENERATE DUMMY DATA ---\n",
    "# We create 5 samples with 1 feature each.\n",
    "# The relationship is roughly y = 2x + 1\n",
    "X_train = np.array([[1], [2], [3], [4], [5]]) \n",
    "y_train = np.array([3, 5, 7, 9, 11]) \n",
    "\n",
    "# --- STEP 2: CONFIGURE THE MODEL ---\n",
    "# Here is where you change variables to your liking:\n",
    "\n",
    "model = RegularizedLinearRegression(\n",
    "    learning_rate=0.01,   # How big of a step we take. \n",
    "                          # If too small: Model learns too slowly.\n",
    "                          # If too big: Model might overshoot the answer.\n",
    "    \n",
    "    n_iterations=1000,    # How many times the model sees the data.\n",
    "                          # More iterations = better fit, but takes longer.\n",
    "    \n",
    "    lambda_param=0.5,     # THE REGULARISATION STRENGTH.\n",
    "                          # 0.0 = No regularisation (Standard Regression).\n",
    "                          # 100.0 = Huge penalty (Weights will be crushed to near 0).\n",
    "                          # Try changing this to see how weights shrink!\n",
    "    \n",
    "    mode='l2'             # THE TYPE OF PENALTY.\n",
    "                          # 'l2' (Ridge) = Good for general stability.\n",
    "                          # 'l1' (Lasso) = Good for feature selection.\n",
    ")\n",
    "\n",
    "# --- STEP 3: TRAIN THE MODEL ---\n",
    "# The .fit() method runs the gradient descent loop we wrote earlier.\n",
    "print(\"Training model...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# --- STEP 4: INSPECT RESULTS ---\n",
    "# Let's see what weights the model learned.\n",
    "print(f\"Final Weights: {model.weights}\")\n",
    "print(f\"Final Bias: {model.bias}\")\n",
    "\n",
    "# --- STEP 5: MAKE A PREDICTION ---\n",
    "# Predict the value for a new input, x = 6.\n",
    "# We expect the answer to be around 13 (since y = 2x + 1).\n",
    "X_test = np.array([[6]])\n",
    "prediction = model.predict(X_test)\n",
    "\n",
    "print(f\"Prediction for input 6: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61909fab-d233-403f-b9ba-9a1b908ebb82",
   "metadata": {},
   "source": [
    "1. The Equation the Model Found\n",
    "The model learned the relationship between $X$ and $y$ is:$$y = 1.86x + 1.39$$\n",
    "Final Weights (1.86): This is the slope.\n",
    "Final Bias (1.39): This is the y-intercept (where the line hits the vertical axis).\n",
    "2. Why isn't it exactly $y = 2x + 1$?\n",
    "You might remember the \"true\" data followed the pattern $y = 2x + 1$.\n",
    "True Slope: 2.0\n",
    "My Slope: 1.86\n",
    "Why is it lower?\n",
    "Because of Regularisation!You set lambda_param=0.5. This \"tax\" punished the model for having a weight of 2.0. To lower its \"tax bill,\" the model shrank the weight down to 1.86. This proves the regularisation code is actually doing its job!\n",
    "3. The Prediction Check\n",
    "The model was asked: \"If input is 6, what is the output?\"The Math: $1.86(6) + 1.39 \\approx 12.55$The \"Perfect\" Answer: $2(6) + 1 = 13$The answer is slightly off (12.55 vs 13), but that is the trade-off. You accepted a slightly less accurate line in exchange for a \"simpler\" (smaller weight) model. In the real world, this \"simpler\" model is much safer against noise and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeab69da-8810-4d60-b6e0-f7317cfe60fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
